{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2장 파이썬으로 시작하는 크롤링/스크레이핑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 웹 페이지 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'http.client.HTTPResponse'>\n",
      "200\n",
      "text/html; charset=UTF-8\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "f = urlopen('http://hanbit.co.kr')\n",
    "\n",
    "#urlopen() 함수는 HTTPResponse 자료형의 객체를 반환\n",
    "#이 객체는 파일 객체이므로 open() 함수로 반환되는 파일 객체처럼 다루면 됨\n",
    "print(type(f))\n",
    "\n",
    "\n",
    "#HTTP 연결은 자동으로 닫히므로 따로 close() 함수를 호출하지 않아도 됨\n",
    "# print(f.read())\n",
    "print(f.status) #상태 코드 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 문자 코드 다루기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HTTP 헤더에서 인코딩 방식 추출하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f.read의 타입:  <class 'bytes'>\n",
      "text/html; charset=UTF-8\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding:  utf-8\n"
     ]
    }
   ],
   "source": [
    "#read() 메서드로 HTTP 응답 본문(bytes 자료형)을 추출\n",
    "print(\"f.read의 타입: \", type(f.read()))\n",
    "\n",
    "#HTTPResponse.read() 메서드로 추출할 수 있는 응답 본문의 값은 bytes 자료형이므로 문자열로 다루려면 문자 코드를 지정해서 디코딩해야 함\n",
    "#HTTP 헤더를 참조해서 적절한 인코딩 방식으로 디코딩 필요\n",
    "print(f.getheader('Content-Type')) #HTTP 헤더의 값을 추출\n",
    "print()\n",
    "\n",
    "\n",
    "#===========================================\n",
    "#인코딩 방슥 추출 + 디코딩\n",
    "#===========================================\n",
    "import sys\n",
    "from urllib.request import urlopen\n",
    "f = urlopen('http://www.hanbit.co.kr/store/books/full_book_list.html')\n",
    "\n",
    "#HTTP 헤더를 기반으로 인코딩 방식 추출\n",
    "encoding = f.info().get_content_charset(failobj='utf-8') #명시되어 있지 않을 경우, utf-8 사용\n",
    "#인코딩 방식을 표준 오류에 출력\n",
    "print('encoding: ', encoding, file=sys.stderr) \n",
    "\n",
    "#추출한 인코딩 방식으로 디코딩\n",
    "text = f.read().decode(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**meta 태그에서 인코딩 방식 추출하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "encoding:  utf-8\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "from urllib.request import urlopen\n",
    "\n",
    "f = urlopen('http://www.hanbit.co.kr/store/books/full_book_list.html')\n",
    "bytes_content = f.read() #bytes 자료형의 응답 본문을 일단 변수에 저장\n",
    "\n",
    "#charset은 HTML의 앞부분에 적혀 있는 경우가 많으므로\n",
    "#응답 본문의 앞부분 1024바이트를 ASCII 문자로 디코딩해 둔다\n",
    "#ASCII 범위 이외의 문자는 U+FFFD(REPLACEMENT CHARACTER)로 변환되어 예외가 발생하지 않는다\n",
    "scanned_text = bytes_content[:1024].decode('ascii', errors='replace')\n",
    "\n",
    "#디코딩한 문자열에서 정규 표현식으로 charset 값을 추출한다\n",
    "match = re.search(r'charset=[\"\\']?([\\w-]+)', scanned_text)\n",
    "if match:\n",
    "    encoding = match.group(1)\n",
    "else:\n",
    "    #charset이 명시되어 있지 않으면 UTF-8 사용\n",
    "    encoding = 'utf-8'\n",
    "    \n",
    "#추출한 인코딩을 표준 오류에 출력\n",
    "print('encoding: ', encoding, file=sys.stderr)\n",
    "\n",
    "#추출한 인코딩으로 다시 디코딩\n",
    "text = bytes_content.decode(encoding)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 웹페이지에서 데이터 추출하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 정규 표현식: HTML을 단순한 문자열로 취급하고, 필요한 부분 추출\n",
    "\n",
    "* XML Parser: XTML 태그를 분석하고, 필요한 부분 추출 / HTML을 바로 넣어서 분석할 수는 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.1 정규 표현식으로 스크레이핑하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 3), match='abc'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#re.search() 함수를 사용하면 두 번째 매개변수의 문자열이 첫 번째 매개변수의 정규 표현식에 맞는지 확인할 수 있음\n",
    "#맞는 경우 Match 객체를 반환 / 맞지 않으면 None을 반환\n",
    "re.search(r'a.*c', 'abc123DEF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#정규 표현식에 맞지 않으므로 None 반환\n",
    "re.search(r'a.*d', 'abc123DEF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 7), match='abc123D'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#세 번째 매개변수로 옵션 지정\n",
    "re.search(r'a.*d', 'abc123DEF', re.IGNORECASE) #대소문자 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n",
      "b\n"
     ]
    }
   ],
   "source": [
    "#Match 객체의 group() 메서드로 일치한 값을 추출\n",
    "#매개변수에 0을 지정하면 매치된 모든 값을 반환\n",
    "#매개변수에 1 이상의 숫자를 지정하면 정규 표현식에서 ()로 감싼 부분에 해당하는 값을 추출\n",
    "#1: 1번째 그룹, 2: 2번째 그룹\n",
    "m = re.search(r'a(.*)c', 'abc123DEFF')\n",
    "print(m.group(0))\n",
    "print(m.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'pen']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#re.findall() 함수를 사용하면 정규 표현식에 맞는 모든 부분을 추출할 수 있음\n",
    "#\\w: 유니코드로 글자 비교\n",
    "#\\s: 공백 문자 추출\n",
    "re.findall(r'\\w{2,}', 'This is a pen') #2글자 이상의 단어를 모두 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That That a That'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#re.sub() 함수 사용하면 정규 표현식에 맞는 부분을 바꿀 수 있음\n",
    "re.sub(r'\\w{2,}', 'That', 'This is a pen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: http://www.hanbit.co.kr/store/books/look.php?p_code=B7126889829\n",
      "title: 핸즈온 비지도 학습\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "\n",
    "html = text\n",
    "\n",
    "#re.findall()을 사용해 도서 하나에 해당하는 HTML 추출\n",
    "for partial_html in re.findall(r'<td class=\"left\"><a.*?</td>', html, re.DOTALL):\n",
    "    #도서의 url 추출\n",
    "    url = re.search(r'<a href=\"(.*?)\">', partial_html).group(1)\n",
    "    url = 'http://www.hanbit.co.kr' + url\n",
    "    #태그를 제거해서 도서의 제목 추출\n",
    "    title = re.sub(r'<.*?>', '', partial_html)\n",
    "    title = unescape(title)\n",
    "    print('url:', url)\n",
    "    print('title:', title)\n",
    "    print('---')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.2 XML(RSS) 스크레이핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-3aaba56cb555>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#parse() 함수로 파일을 읽어 들이고 ElementTree 객체를 만듦\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#getroot() 메서드로 XML의 루트 요소를 추출\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\xml\\etree\\ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(source, parser)\u001b[0m\n\u001b[0;32m   1195\u001b[0m     \"\"\"\n\u001b[0;32m   1196\u001b[0m     \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElementTree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1197\u001b[1;33m     \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1198\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\xml\\etree\\ElementTree.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, source, parser)\u001b[0m\n\u001b[0;32m    585\u001b[0m         \u001b[0mclose_source\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"read\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 587\u001b[1;33m             \u001b[0msource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    588\u001b[0m             \u001b[0mclose_source\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109'"
     ]
    }
   ],
   "source": [
    "#ElementTree모듈 읽어들이기\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "#parse() 함수로 파일을 읽어 들이고 ElementTree 객체를 만듦\n",
    "tree = ElementTree.parse('rss.xml')\n",
    "\n",
    "#getroot() 메서드로 XML의 루트 요소를 추출\n",
    "root = tree.getroot()\n",
    "\n",
    "#findall() 메서드로 요소 목록을 추출\n",
    "#태그 찾기\n",
    "for item in root.findall('channel/item/description/body/location/data'):\n",
    "    #find() 메서드로 요소를 찾고 text 속성으로 값 추출\n",
    "    tm_df = item.find('tmEf').text\n",
    "    tmn = item.find('tmn').text\n",
    "    tmx = item.find('tmx').text\n",
    "    wf = item.find('wf').text\n",
    "    \n",
    "# http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId=109"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 데이터 저장하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.2 JSON 형식으로 저장히기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"rank\": 1, \"city\": \"\\uc0c1\\ud558\\uc774\", \"population\": 24150000}, {\"rank\": 2, \"city\": \"\\uce74\\ub77c\\uce58\", \"population\": 23500000}, {\"rank\": 3, \"city\": \"\\ubca0\\uc774\\uc9d5\", \"population\": 21516000}, {\"rank\": 4, \"city\": \"\\ud150\\uc9c4\", \"population\": 14722100}, {\"rank\": 5, \"city\": \"\\uc774\\uc2a4\\ud0c4\\ubd88\", \"population\": 14160467}]\n",
      "[\n",
      "  {\n",
      "    \"rank\": 1,\n",
      "    \"city\": \"상하이\",\n",
      "    \"population\": 24150000\n",
      "  },\n",
      "  {\n",
      "    \"rank\": 2,\n",
      "    \"city\": \"카라치\",\n",
      "    \"population\": 23500000\n",
      "  },\n",
      "  {\n",
      "    \"rank\": 3,\n",
      "    \"city\": \"베이징\",\n",
      "    \"population\": 21516000\n",
      "  },\n",
      "  {\n",
      "    \"rank\": 4,\n",
      "    \"city\": \"텐진\",\n",
      "    \"population\": 14722100\n",
      "  },\n",
      "  {\n",
      "    \"rank\": 5,\n",
      "    \"city\": \"이스탄불\",\n",
      "    \"population\": 14160467\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "cities = [\n",
    "    {'rank':1, 'city':'상하이', 'population':24150000},\n",
    "    {'rank':2, 'city':'카라치', 'population':23500000},\n",
    "    {'rank':3, 'city':'베이징', 'population':21516000},\n",
    "    {'rank':4, 'city':'텐진', 'population':14722100},\n",
    "    {'rank':5, 'city':'이스탄불', 'population':14160467},\n",
    "]\n",
    "\n",
    "print(json.dumps(cities))\n",
    "print(json.dumps(cities, ensure_ascii=False, indent=2))\n",
    "\n",
    "#파일에 저장\n",
    "with open('top_cities.json', 'w') as f:\n",
    "    json.dump(cities, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3장 주요 라이브러리 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 웹페이지 간단하게 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get() 함수 반환값: <class 'requests.models.Response'>\n",
      "HTTP 상태 코드: 200\n",
      "HTTP 헤더: text/html; charset=UTF-8\n",
      "encoding: UTF-8\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "#get() 함수로 웹 페이지 추출\n",
    "r = requests.get('http://hanbit.co.kr')\n",
    "print('get() 함수 반환값:', type(r))\n",
    "print('HTTP 상태 코드:', r.status_code)\n",
    "print('HTTP 헤더:', r.headers['content-type'])\n",
    "print('encoding:', r.encoding)\n",
    "# print(r.text)\n",
    "# print(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('http://weather.livedoor.com/forecast/webservice/json/v1?city=130010')\n",
    "# r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POST 메서드로 전송\n",
    "#키워드 매개변수 data에 딕셔너리를 지정하면 HTML 입력 양식처럼 전송됨\n",
    "r = requests.post('http://httpbin.org/post', data={'key1':'value1'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 HTML 스크레이핑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 XPath와 CSS 선택자\n",
    "\n",
    "* XPath: XTML의 특정 요소를 지정할 때 사용하는 언어\n",
    "    * ex) //body/h1 : body 요소의 직접적인 자식 중 h1 태그 선택\n",
    "    \n",
    "    \n",
    "* CSS 선택자: CSS로 요소를 디자인할 때 사용하는 표기 방법\n",
    "    * ex) body > h1 : body 요소의 직접적인 자식 중 h1 태그 선택"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 lxml로 스크레이핑하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cssselect\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
      "Installing collected packages: cssselect\n",
      "Successfully installed cssselect-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip indysll lxml\n",
    "!pip install cssselect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'lxml.etree._ElementTree'>\n",
      "<class 'lxml.html.HtmlElement'>\n",
      "h1 tag: h1\n",
      "h1.text: None\n",
      "h1.id: None\n",
      "h1.attrib: {}\n",
      "h1.getparent: <Element div at 0x150cbe314f8>\n"
     ]
    }
   ],
   "source": [
    "import lxml.html\n",
    "\n",
    "tree = lxml.html.parse('full_book_list.html') #parse() 함수로 파일 경로를 지정할 수 있음\n",
    "print(type(tree))\n",
    "# tree = lxml.html.parse('http://example.com/') #parse() 함수로 URL 직접 지정할 수도 있지만 미세한 설정을 따로 할 수 없으므로 추천은 X\n",
    "\n",
    "#파일 객체를 지정해서 파싱할 수도 있음\n",
    "# from urllib.request import urlopen\n",
    "# tree = lxml.html.parse(urlopen('http://example.com/'))\n",
    "\n",
    "html = tree.getroot() #getroot() 메서드로 html 루트 요소의 HtmlElement 객체를 추출할 수 있음\n",
    "print(type(html))\n",
    "\n",
    "#HtmlElement의 xpath() 메서드로 XPath와 일치하는 요소 목록을 추출할 수 있음\n",
    "# html.xpath('//li')\n",
    "#HtmlElement의 cssselect() 메서드로 선택자와 일치하는 요소 목록 추출할 수 있음\n",
    "# html.cssselect('li')\n",
    "\n",
    "h1 = html.xpath('//h1')[0]\n",
    "print('h1 tag:', h1.tag)\n",
    "print('h1.text:', h1.text)\n",
    "print('h1.id:', h1.get('id'))\n",
    "print('h1.attrib:', h1.attrib)\n",
    "print('h1.getparent:', h1.getparent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ None\n"
     ]
    }
   ],
   "source": [
    "import lxml.html\n",
    "\n",
    "tree = lxml.html.parse('hide on bush')\n",
    "html = tree.getroot()\n",
    "\n",
    "for a in html.cssselect('a'):\n",
    "    #href 속성과 글자 추출\n",
    "    print(a.get('href'), a.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Beautiful Soup로 스크레이핑하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\lg\\anaconda3\\lib\\site-packages (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from beautifulsoup4) (1.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('hide on bush', encoding='UTF-8') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "    \n",
    "for a in soup.find_all('a'):\n",
    "    print(a.get('href'), a.text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 파이썬으로 크롤러 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7.1 목록 페이지에서 퍼머 링크 목록 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.hanbit.co.kr/store/books/look.php?p_code=B7126889829\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "response = requests.get('http://www.hanbit.co.kr/store/books/new_book_list.html')\n",
    "root = lxml.html.fromstring(response.content)\n",
    "\n",
    "#모든 링크를 절대 URL로 변환\n",
    "root.make_links_absolute(response.url)\n",
    "\n",
    "for a in root.cssselect('.view_box .book_tit a'):\n",
    "    url = a.get('href')\n",
    "    print(url)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'url': 'https://www.hanbit.co.kr/store/books/look.php?p_code=B7126889829', 'title': '핸즈온 비지도 학습', 'price': '30,600', 'content': ['\\r\\n\\t\\tCHAPTER 0 서문\\r\\n', '0.1 머신러닝의 역사', '0.2 인공지능의 귀환, 왜 지금인가?', '0.3 응용 인공지능의 출현', '0.4 지난 20년간 응용 인공지능 주요 성과', '0.5 좁은 인공지능부터 범용 인공지능까지', '0.6 목표와 접근방식', '0.7 이 책의 구성', '0.8 예제 다운로드 안내', '\\r\\n\\t\\tPART 1 비지도 학습 개요\\xa0\\r\\n', '\\r\\n\\t\\tCHAPTER 1 머신러닝 생태계와 비지도 학습\\xa0\\r\\n', '1.1 머신러닝 기본 용어 이해하기', '1.2 규칙 기반과 머신러닝 비교하기', '1.3 지도 학습과 비지도 학습 비교하기', '1.4 비지도 학습을 사용해 머신러닝 솔루션 개선하기', '1.5 지도 학습 알고리즘 자세히 살펴보기\\xa0', '1.6 비지도 학습 알고리즘 자세히 살펴보기\\xa0', '1.7 비지도 학습을 활용한 강화 학습\\xa0', '1.8 준지도 학습\\xa0', '1.9 비지도 학습의 성공적인 응용 사례\\xa0', '1.10 마치며\\xa0', '\\r\\n\\t\\tCHAPTER 2 머신러닝 프로젝트 A to Z\\xa0\\r\\n', '2.1 환경 설정\\xa0', '2.2 데이터 개요', '2.3 데이터 준비하기', '2.4 모델 준비하기', '2.5 머신러닝 모델(1)', '2.6 평가 지표', '2.7. 머신러닝 모델(2)', '2.8 테스트 데이터셋으로 4가지 모델 평가하기', '2.9 앙상블', '2.10 최종 모델 선택하기', '2.11 프로덕션 파이프라인', '2.12 마치며', '\\r\\n\\t\\tPART 2 사이킷런을 사용한 비지도 학습 모델\\xa0\\r\\n', '\\r\\n\\t\\tCHAPTER 3 차원 축소\\xa0\\r\\n', '3.1 차원 축소에 대한 동기 부여', '3.2 차원 축소 알고리즘', '3.3 PCA', '3.4 SVD', '3.5 랜덤 투영', '3.6 Isomap', '3.7 MDS', '3.8 LLE', '3.9 t-SNE', '3.10 사전 학습\\xa0', '3.11 ICA', '3.12 마치며', '\\r\\n\\t\\tCHAPTER 4 이상치 탐지\\xa0\\r\\n', '4.1 신용카드 사기 탐지', '4.2 일반 PCA를 활용한 이상치 탐지', '4.3 희소 PCA를 활용한 이상치 탐지', '4.4 커널 PCA를 활용한 이상치 탐지', '4.5 GRP를 활용한 이상치 탐지', '4.6 SRP를 활용한 이상치 탐지', '4.7 비선형 이상치 탐지', '4.8 사전 학습을 활용한 이상치 탐지', '4.9 ICA를 활용한 이상치 탐지', '4.10 테스트셋으로 이상치 탐지 성능 평가', '4.11 마치며', '\\r\\n\\t\\tCHAPTER 5 클러스터링\\xa0\\r\\n', '5.1 MNIST 데이터셋', '5.2 클러스터링 알고리즘', '5.3 k-평균', '5.4 계층적 클러스터링', '5.5 DBSCAN 개요', '5.6 마치며', '\\r\\n\\t\\tCHAPTER 6 그룹 세분화\\xa0\\r\\n', '6.1 랜딩 클럽 데이터', '6.2 군집 적합도 검정', '6.3 k-평균 클러스터링 응용 프로그램', '6.4 계층적 클러스터링 응용 프로그램', '6.5 HDBSCAN 응용 프로그램', '6.6 마치며', '\\r\\n\\t\\tPART 3 텐서플로와 케라스를 사용한 비지도 학습 모델\\xa0\\r\\n', '\\r\\n\\t\\tCHAPTER 7 오토인코더\\xa0\\r\\n', '7.1 신경망', '7.2 오토인코더: 인코더와 디코더', '7.3 과소완전 오토인코더', '7.4 과대완전 오토인코더', '7.5 고밀도 vs 희소 오토인코더', '7.6 노이즈 제거 오토인코더', '7.7 변분 오토인코더', '7.8 마치며', '\\r\\n\\t\\tCHAPTER 8 핸즈온 오토인코더\\xa0\\r\\n', '8.1 데이터 준비', '8.2 오토인코더의 구성 요소', '8.3 활성화 함수', '8.4 첫 번째 오토인코더', '8.5 선형 활성화 함수로 구성된 2-계층 과소완전 오토인코더', '8.6 비선형 오토인코더', '8.7 선형 활성화 함수로 구성된 과대완전 오토인코더', '8.8 선형 활성화 함수와 드롭아웃으로 구성된 과대완전 오토인코더', '8.9 선형 활성화 함수로 구성된 희소 과대완전 오토인코더', '8.10 선형 활성화 함수와 드롭아웃으로 구성된 희소 과대완전 오토인코더', '8.11 노이즈 데이터셋 생성', '8.12 노이즈 제거 오토인코더', '8.13 마치며', '\\r\\n\\t\\tCHAPTER 9 준지도 학습\\xa0\\r\\n', '9.1 데이터 준비', '9.2 지도 학습 모델', '9.3 비지도 학습 모델', '9.4 준지도 학습 모델', '9.5 지도 학습과 비지도 학습의 강력함', '9.6 마치며', '\\r\\n\\t\\tPART 4 텐서플로와 케라스를 사용한 심층 비지도 학습\\xa0\\r\\n', '\\r\\n\\t\\tCHAPTER 10 RBM을 사용한 추천 시스템\\xa0\\r\\n', '10.1 볼츠만 머신', '10.2 추천 시스템', '10.3 무비렌즈 데이터셋', '10.4 행렬 인수분해', '10.5 RBM을 사용한 협업 필터링', '10.6 마치며', '\\r\\n\\t\\tCHAPTER 11 DBN을 사용한 피처 추출\\xa0\\r\\n', '11.1 심층 신뢰 신경망 자세히 살펴보기', '11.2 MNIST 이미지 분류하기', '11.3 RBM', '11.4 DBN을 위한 세 RBM 훈련', '11.5 전체 DBN', '11.6 비지도 학습이 지도 학습을 개선하는 방법', '11.7 LightGBM을 사용한 이미지 분류기', '11.8 마치며', '\\r\\n\\t\\tCHAPTER 12 GAN\\xa0\\r\\n', '12.1 GAN의 개념', '12.2 DCGAN', '12.3 CNN', '12.4 DCGAN으로 돌아가기', '12.5 MNIST DCGAN 실행', '12.6 마치며', '\\r\\n\\t\\tCHAPTER 13 시계열 클러스터링\\xa0\\r\\n', '13.1 심전도 데이터', '13.2 시계열 클러스터링 접근 방법', '13.3 ECGFiveDays 데이터셋에서 k-Shape을 사용한 시계열 클러스터링', '13.4 ECG5000 데이터셋에서 k-Shape을 사용한 시계열 클러스터링', '13.5 ECG5000 데이터셋에서 k-평균을 사용한 시계열 클러스터링', '13.6 ECG5000 데이터셋에서 HDBSCAN을 사용한 시계열 클러스터링', '13.7 시계열 클러스터링 알고리즘 비교', '13.8 마치며', '\\r\\n\\t\\tCHAPTER 14 결론\\xa0\\r\\n', '14.1 지도 학습', '14.2 비지도 학습', '14.3 강화 학습', '14.4 오늘날 가장 유망한 비지도 학습 분야', '14.5 비지도 학습의 미래', '14.6 마치며']}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "import time\n",
    "import re\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    크롤러의 메인 처리\n",
    "    \"\"\"\n",
    "    \n",
    "    #여러 페이지에서 크롤링할 것이므로 Session을 사용\n",
    "    session = requests.Session()\n",
    "    \n",
    "    #scrape_list_page() 함수를 호출해서 제너레이터 추출\n",
    "    response = session.get('http://www.hanbit.co.kr/store/books/new_book_list.html')\n",
    "    urls = scrape_list_page(response)\n",
    "    \n",
    "    for url in urls:\n",
    "        time.sleep(1)\n",
    "        response = session.get(url) #Session을 사용해 상세 페이지 추출\n",
    "        ebook = scrape_detail_page(response) #상세 페이지에서 상세 정보 추출\n",
    "        print(ebook)\n",
    "        break\n",
    "        \n",
    "def scrape_list_page(response):\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    root.make_links_absolute(response.url)\n",
    "    \n",
    "    for a in root.cssselect('.view_box .book_tit a'):\n",
    "        url = a.get('href')\n",
    "        #yield 구문으로 제너레이터의 요소 반환\n",
    "        yield url\n",
    "        \n",
    "def scrape_detail_page(response):\n",
    "    \"\"\"\n",
    "    상세 페이지의 Response에서 책 정보를 dict로 추출\n",
    "    \"\"\"\n",
    "    root = lxml.html.fromstring(response.content)\n",
    "    ebook = {\n",
    "        'url':response.url,\n",
    "        'title':root.cssselect('.store_product_info_box h3')[0].text_content(),\n",
    "        'price':root.cssselect('.pbr strong')[0].text_content(),\n",
    "        'content':[p.text_content()\\\n",
    "                  for p in root.cssselect('#tabs_3 .hanbit_edit_view p')\n",
    "                  if normalize_spaces(p.text_content()) != '']\n",
    "    }\n",
    "    \n",
    "    return ebook\n",
    "\n",
    "def normalize_spaces(s):\n",
    "    \"\"\"\n",
    "    연결돼 있는 공백을 하나의 공백으로 변경\n",
    "    \"\"\"\n",
    "    \n",
    "    return re.sub(r's+', ' ', s).strip() #strip(): 앞뒤 공백 자르기\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HtmlElement' object has no attribute 'attrs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-140-404293cf176f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#     print(url)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HtmlElement' object has no attribute 'attrs'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "\n",
    "# response = requests.get('https://your.gg/kr/profile/Hide%20on%20bush')\n",
    "response = requests.get('https://your.gg/kr/profile/Hide%20on%20bush/match/4504741686')\n",
    "root = lxml.html.fromstring(response.content)\n",
    "\n",
    "#모든 링크를 절대 URL로 변환\n",
    "root.make_links_absolute(response.url)\n",
    "\n",
    "for a in root.cssselect('.d-flex.flex-column span'):\n",
    "    url = a.get('href')\n",
    "    cs = a.get('class')\n",
    "    text = a.get('text')\n",
    "#     print(url)\n",
    "    print(a.attrs)\n",
    "    print(cs)\n",
    "    print(text)\n",
    "#     breakbody > div > div.container-fluid.page-body-wrapper > div > div > div.d-flex.flex-column > div.row.mt-lg-3.mt-3 > div:nth-child(2) > div > div > div > div.d-flex.flex-column.justify-content-between.h-100 > div:nth-child(6) > div:nth-child(5) > span.text-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<re.Match object; span=(2, 8), match='python'>\n",
      "python\n",
      "['life', 'is', 'too', 'short']\n",
      "<callable_iterator object at 0x00000150CF4A6048>\n",
      "<re.Match object; span=(0, 4), match='life'>\n",
      "<re.Match object; span=(5, 7), match='is'>\n",
      "<re.Match object; span=(8, 11), match='too'>\n",
      "<re.Match object; span=(12, 17), match='short'>\n"
     ]
    }
   ],
   "source": [
    "#regular expression\n",
    "\n",
    "import re\n",
    "\n",
    "p = re.compile('[a-z]+')\n",
    "\n",
    "m = p.match(\"3 python\")\n",
    "print(m)\n",
    "\n",
    "s = p.search(\"3 python hello\")\n",
    "print(s)\n",
    "print(s.group())\n",
    "\n",
    "result = p.findall(\"life is too short\")\n",
    "print(result)\n",
    "\n",
    "result = p.finditer(\"life is too short\")\n",
    "print(result)\n",
    "\n",
    "for r in result: \n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4504741686', 'Solo', '48m ago', 'Win', '24 min', 'LV15', '7.6', 'S', '4:6', '3', '2', '21', '183(7.6)', '55%', 'huntiand1', 'ke ai ke ai', 'Hide on bush', 'xxxss', '참외 소스 하하', 'impupupu', 'AF Burry', '스누피2', '쌍수도', '홈런볼 다 내꺼']\n",
      "https://ddragon.leagueoflegends.com/cdn/10.14.1/img/spell/SummonerTeleport.png\n",
      "https://ddragon.leagueoflegends.com/cdn/10.14.1/img/spell/SummonerFlash.png\n",
      "https://ddragon.leagueoflegends.com/cdn/img/perk-images/Styles/7203_Whimsy.png\n",
      "https://ddragon.leagueoflegends.com/cdn/img/perk-images/Styles/7202_Sorcery.png\n",
      "https://ddragon.leagueoflegends.com/cdn/10.14.1/img/champion/Galio.png\n",
      "/v4/media/tran-Mid.svg\n",
      "https://ddragon.leagueoflegends.com/cdn/10.14.1/img/champion/Camille.png\n",
      "https://ddragon.leagueoflegends.com/cdn/10.14.1/img/champion/LeeSin.png\n",
      "https://ddragon.leagueoflegends.com/cdn/10.14.1/img/champion/Galio.png\n",
      "https://ddragon.leagueoflegends.com/cdn/10.14.1/img/champion/Kaisa.png\n",
      "https://ddragon.leagueoflegends.com/cdn/10.14.1/img/champion/Sett.png\n",
      "https://ddragon.leagueoflegends.com/cdn/10.14.1/img/champion/Aatrox.png\n",
      "https://ddragon.leagueoflegends.com/cdn/10.14.1/img/champion/RekSai.png\n",
      "https://ddragon.leagueoflegends.com/cdn/10.14.1/img/champion/Sylas.png\n",
      "https://ddragon.leagueoflegends.com/cdn/10.14.1/img/champion/Kalista.png\n",
      "https://ddragon.leagueoflegends.com/cdn/10.14.1/img/champion/Thresh.png\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "with open('hide on bush', encoding='UTF-8') as f:\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "    \n",
    "#평가등급 가져오기\n",
    "# for div in soup.find_all('div', attrs={'id':'profileGraphAllPc'}):\n",
    "#     print(div.get('data-json'))\n",
    "\n",
    "match_info_keys = ['id', 'type', 'when', 'result', 'time', 'lv', 'rating', 'luck', 'laning', 'kill', 'death', 'assistant', 'CS', 'KP', '1_top', '1_jug', '1_mid', '1_ad', '1_sup', '2_top', '2_jug', '2_mid', '2_ad', '2_sup']\n",
    "hide_on_bush = pd.DataFrame(columns=match_info_keys)\n",
    "\n",
    "for div in soup.select('div.card.gg-matchlist.gg-a.py-3.px-lg-3'):\n",
    "    data_href = div.get('data-href')\n",
    "    p = re.compile('[0-9]+')\n",
    "    match_id = p.search(data_href).group()\n",
    "#     print('match id:', p.search(data_href).group())\n",
    "    \n",
    "    for div in div.find_all('div'):\n",
    "        match_info = re.sub(r'\\s{5,}', '\\n', div.text)\n",
    "        match_info = re.sub(r'\\n{2,}', '\\n', match_info).strip()\n",
    "        match_info = match_info.split('\\n')\n",
    "\n",
    "        for i in range(6, 14):\n",
    "            del match_info[i]\n",
    "            \n",
    "        match_info = [match_id] + match_info\n",
    "        \n",
    "        print(match_info)\n",
    "        break\n",
    "    \n",
    "    for img in div.find_all('img'):\n",
    "        print(img.get('src'))\n",
    "    break\n",
    "#     hide_on_bush = hide_on_bush.append(dict((key, value) for key, value in zip(match_info_keys, match_info)), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "hide_on_bush.to_csv('./hide_on_bush.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
